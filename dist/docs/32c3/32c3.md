## text 

    Setting up an environment to analyze Tor data is no trivial task. Access to the 
    raw data is only feasable programatically through special libraries. Also the 
    data is collected on a node per hour basis whereas most of the time one will be 
    interested in certain kinds of nodes within a certain period of time. Therefor a 
    lot of aggregation has to be performed upfront before you can even start to ask 
    the questions that you came for. Not to mention that the shere amount of data 
    might block your notebook for days and eat up your harddrive on the way. 
    
    The analytics project plans to provide a small set of tools to ease these 
    problems:
    - a converter from raw CollecTor data to JSON and more performant Avro/Parquet
    - a Big Data setup providing popular interfaces like MapReduce, R and SQL
      and an easy migration path to scale huge tasks from notebook to cloud 
    - a collection of pre-aggregated datasets and aggregation scripts.
    With these tools it should become much more feasable to work with Tor data and 
    perform analytics as need arises, without too much fuzz and effort upfront. 
    
    This is still work in progress, so please allow a few more months.


## notes
    
    analyticServer project +++ WORK IN PROGRESS +++
    
    case:   analyzing Tor data needs a lot of work upfront 
            for many tasks probably a prohibitively high barrier
    
    goal:   make data about the Tor network easier to access and analyze 
            [ support exploration of patterns, detection of dependencies ] 
            given the size and structure of the Tor data set this is a "Big Data" task
    
    tool:   1   provide data in formats ready to consume with standard desktop tools 
            2   pre-aggregate often used aspects of the data 
            3   ease use of big data infrastructure, either locally or in the cloud
    
    
    1  
    data about Tor comes in a special format optimized for transport over the network 
    we already provide libraries to access it programatically (metrics-lib, stem, zooosh) 
    but it is not supported by any common software out of the box
    
    therefor we are developing a converter from raw Tor data to more popular formats:
    
    ->  JSON 
        - everbodys darling these days
        - supported by popular tools like MongoDB database, 
          Tableau data visualization/analysis, 
        - ubiquitious on the web
    
    ->  Avro/Parquet 
        - very efficient storage and access
        - optimized for use in Big Data installations
    
    the converter will be available as a commandline converter tool
    we’ll probably also offer readily converted data sets
    
    
    2  
    Tor data is collected per server, hourly 
    to find patterns it has to be aggregated over characteristics and timespans 
    that is tedious
    some of these aggregations are very common 
    we’ll strive to provide them as scripts and/or downloads 
    your aggregation scripts will be welcome too
    
    
    3  
    you don’t have to operate a giga-node cluster to do Big Data analytics 
    instead 
    - the JSON data format is supported by many desktop tools and everywhere on the web 
    - also Big Data softwares can be used without advanced sysadmin skills
    
    the following softwares are all doubleclick-installable as "embedded"
    single-node applications on your notebook 
    
    Apache Hadoop as mapReduce engine 
    Apache Spark for R/Java/Scala/Python interfaces to the data 
    Apache Drill to work on the data with plain old SQL
    Apache HBase for fast per-relay lookup 
    
    one of them might be all you need 
    
    - when you’ve set up a Big Data project locally and
    run into the hardware limitations you can move it with little
    effort to a bigger server or a cloud provider this is the plan.
    
    the concept is ambitious, experience shaky, the converter in
    alpha, details tedious. don’t tweet about it yet.
    

