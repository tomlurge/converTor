strange things happn with parquet conversion    
is this because we don't use hadoop fs?

sometimes the process doesn't finish for too long a time    
sometimes it finishes like this:

   /Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/bin/java -Xmx4g -Didea.launcher.port=7542 "-Didea.launcher.bin.path=/Applications/IntelliJ IDEA 15.app/Contents/bin" -Dfile.encoding=UTF-8 -classpath "/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/htmlconverter.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/lib/javafx-doclet.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/lib/tools.jar:/Users/t/Projects/Tor/analyticsServer/converTor/out/production/converTor:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-cli-1.3.1.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-io-2.4.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/descriptor-1.1.0.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/gson-2.5.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/jackson-core-asl-1.9.13.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/jackson-mapper-asl-1.9.13.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/avro-1.8.0.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/avro-tools-1.8.0.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/parquet-avro-1.8.1.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/parquet-column-1.8.1.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/parquet-common-1.8.1.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/parquet-encoding-1.8.1.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-codec-1.5.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/parquet-hadoop-1.8.1.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/parquet-format-2.3.0-incubating.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/parquet-jackson-1.8.1.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/jackson-mapper-asl-1.9.11.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/jackson-core-asl-1.9.11.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/snappy-java-1.1.1.6.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/avro-1.7.6.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/paranamer-2.3.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-compress-1.4.1.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/xz-1.0.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/slf4j-api-1.6.4.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/fastutil-6.5.7.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/hadoop-client-1.1.0.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/hadoop-core-1.1.0.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/xmlenc-0.52.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-io-2.1.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-math-2.1.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-configuration-1.6.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-collections-3.2.1.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-lang-2.4.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-digester-1.8.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-beanutils-1.7.0.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-beanutils-core-1.8.0.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-net-1.4.1.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/oro-2.0.8.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-el-1.0.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/hsqldb-1.8.0.10.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/parquet-avro-examples_2.11-0.2.0.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/scala-library-2.11.7.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/parquet-avro-extra_2.11-0.2.0.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/scala-reflect-2.11.7.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/avro-1.7.4.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/jackson-core-asl-1.8.8.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/jackson-mapper-asl-1.8.8.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/snappy-java-1.0.4.1.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/avro-compiler-1.7.4.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/commons-lang-2.6.jar:/Users/t/Projects/Tor/analyticsServer/converTor/lib/velocity-1.7.jar:/Applications/IntelliJ IDEA 15.app/Contents/lib/idea_rt.jar" com.intellij.rt.execution.application.AppMain converTor.ConverTor -f=parquet -i=/Users/t/Projects/Tor/analyticsServer/converTor/data/in/singles
   format = parquet
   suffix = 
   inPath = /Users/t/Projects/Tor/analyticsServer/converTor/data/in/singles
   outPath = data/out/parquet/
   verbose = false
   compressed = false
   pretty printed JSON = false
   outputFileEnding = .parquet
   
   16/02/27 19:18:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
   SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
   SLF4J: Defaulting to no-operation (NOP) logger implementation
   SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
   
   Deed done, shutting down.
   27.02.2016 19:18:59 INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 76.824
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 49B for [descriptor_type] BINARY: 24 values, 2B raw, 2B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 15B raw, 1B comp}
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 43B for [source] BINARY: 24 values, 8B raw, 8B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 9B raw, 1B comp}
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 41B for [filesize] INT32: 24 values, 8B raw, 8B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 4B raw, 1B comp}
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [start] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [socket] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [connect] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [negotiate] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [request] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [response] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [datarequest] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [dataresponse] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [datacomplete] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 41B for [writebytes] INT32: 24 values, 8B raw, 8B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 4B raw, 1B comp}
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 41B for [readbytes] INT32: 24 values, 8B raw, 8B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 4B raw, 1B comp}
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [didtimeout] BOOLEAN: 24 values, 9B raw, 9B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [dataperc10] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [dataperc20] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [dataperc30] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [dataperc40] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [dataperc50] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [dataperc60] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [dataperc70] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [dataperc80] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [dataperc90] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [launch] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 241B for [used_at] INT64: 24 values, 198B raw, 198B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3.297B for [path, array] BINARY: 72 values, 3.189B raw, 3.189B comp, 1 pages, encodings: [PLAIN, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 641B for [buildtimes, array] INT64: 72 values, 597B raw, 597B comp, 1 pages, encodings: [PLAIN, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 49B for [timeout] INT64: 24 values, 8B raw, 8B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 8B raw, 1B comp}
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 49B for [quantile] DOUBLE: 24 values, 8B raw, 8B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 8B raw, 1B comp}
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 137B for [circ_id] INT32: 24 values, 102B raw, 102B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   27.02.2016 19:19:00 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 137B for [used_by] INT32: 24 values, 102B raw, 102B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
   
   Process finished with exit code 42


