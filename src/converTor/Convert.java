package converTor;

import java.io.*;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.*;

// avro
import org.apache.avro.Schema;
import org.apache.avro.data.Json;
import org.apache.avro.file.DataFileWriter;
import org.apache.avro.io.DatumWriter;
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.io.JsonEncoder;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.avro.specific.SpecificRecord;
import org.apache.avro.specific.SpecificRecordBase;

// avro classes auto-generated from schemas
import converTor.torperf.*;

// parquet-mr
import org.apache.hadoop.fs.Path;
import org.apache.parquet.avro.AvroParquetWriter;

//  metrics-lib
import org.torproject.descriptor.*;

// command line interface
import org.apache.commons.cli.*;

// fileWriter - TODO maybe useful for multifile output


/*
intelliJ keeps stealing my unused import statements :(
import java.io.*;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.*;

// avro
import org.apache.avro.file.DataFileWriter;
import org.apache.avro.io.DatumWriter;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.avro.io.EncoderFactory;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.hadoop.fs.Path;

// avro classes auto-generated from schemas
import converTor.relay.*;
import converTor.relayExtra.*;
import converTor.bridgeExtra.*;
import converTor.relayVote.*;
import converTor.relayConsensus.*;
import converTor.bridgeStatus.*;
import converTor.tordnsel.*;
import converTor.torperf.*;

//  metrics-lib
import org.torproject.descriptor.*;

// command line interface
import org.apache.commons.cli.*;

// fileWriter
import org.apache.commons.io.FileUtils;

 */

public class Convert {


  /*
   * argument defaults
   */

  static boolean verbose = false;
  static boolean compressed = false;
  static boolean pretty = false;
  static boolean json = false;
  static boolean avro = false;
  static boolean parquet = true;
  static String format = "parquet";
  static String inPath = "data/in/";
  static String outPath = "data/out/" + format + "/";
  //  static String prefix = "";
  //  static String name = "result";
  static String suffix = "";
  static int max = 20;
  static String outputFileEnding;


  /*
   * descriptor types and their attributes
   */

  static class DescriptorType {
    String name;         // identifying name of the DescriptorType
    String clasName;     // name of the class (like name, but Capitalized)
    String clasFullName; // name of the class + '.class' extension
    String clasFullPath; // name of the class + '.class' extension
    File clasPath;       // path to avro classes (autogenerated from avro IDL schema)
    Class clas;          // reference to the Class object
    File avscFile;       // avro JSON schema (derived from avro IDL schema)
    Schema avsc;         // parsed JSON schema

    DescriptorType(String name) {
      this.name = name;
      this.clasName = name.substring(0, 1).toUpperCase() + name.substring(1);
      this.clasFullName = clasName + ".class";
      this.clasFullPath = "converTor/" + name + "/" + clasFullName;
      this.clasPath = new File("converTor/" + name + "/" + clasFullName);
      try {
        this.clas = Class.forName("converTor." + name + "." + clasName);
      } catch (ClassNotFoundException c) {
        System.err.println("A ClassNotFoundException was caught: "
                + c.getMessage());
        c.printStackTrace();
      }
      this.avscFile = new File("schema/" + name + ".avsc");
      try {
        Schema.Parser parser = new Schema.Parser();
        this.avsc = parser.parse(avscFile) ;
      } catch (IOException e) {
        e.printStackTrace();
      }
    }
  }
  static DescriptorType relayType          = new DescriptorType("relay");
  static DescriptorType bridgeType         = new DescriptorType("bridge");
  static DescriptorType relayExtraType     = new DescriptorType("relayExtra");
  static DescriptorType bridgeExtraType    = new DescriptorType("bridgeExtra");
  static DescriptorType relayVoteType      = new DescriptorType("relayVote");
  static DescriptorType relayConsensusType = new DescriptorType("relayConsensus");
  static DescriptorType bridgeStatusType   = new DescriptorType("bridgeStatus");
  static DescriptorType tordnselType       = new DescriptorType("tordnsel");
  static DescriptorType torperfType        = new DescriptorType("torperf");



  /*
   *
   *  Read all descriptors in the provided directory and
   *  convert them to the appropriate JSON format.
   *
   */

  public static void main(String[] args) throws IOException {

    // INPUT ARGUMENTS  https://commons.apache.org/proper/commons-cli/usage.html
    Options options = new Options();
    options.addOption("h", "help", false,
            "display this help text");
    options.addOption("f", "format", true,
            "e.g. '-f=json'\n" +
                    "to which serialization format to convert\n" +
                    "defaults to 'parquet'\n" +
                    "possible values are 'avro', 'parquet' and 'json'");
    options.addOption("s", "suffix", true,
            "e.g. '-s=_Suffix'");
    options.addOption("i", "inPath", true,
            "e.g. '-i=/my/data/in/dir'\n" +
                    "from which directory to read data\n" +
                    "defaults to 'data/in/'");
    options.addOption("o", "outPath", true,
            "e.g. '-o=/my/data/out/dir'\n" +
                    "to which directory to write the converted data\n" +
                    "defaults to 'data/out/<format>/'");
    options.addOption("v", "verbose", false,
            "encode 'jagged' maps (objects with non-uniform attribute sets) \n" +
                    "additionally as 'flattened' arrays (with suffix '_FLAT')\n" +
                    "and include all properties with 'null' values (this is only\n" +
                    "relevant for the JSON encoding).\n" +
                    "some SQL engines like Apache Drill require this.");
    //  TODO  implement multi-file compression for JSON
    options.addOption("c", "compressed", false,
            "does generate .gz archive, \n" +
                    "(this is only relevant for JSON)");
    options.addOption("p", "pretty", false,
            "does generate pretty printed JSON output, \n" +
                    "(obviously this is only relevant for JSON too)");
    options.addOption("m", "max", true,
            "maximum file readers to open, \n" +
                    "e.g. '-m=5'\n" +
                    "defaults to 20");
    CommandLineParser parser = new DefaultParser();
    CommandLine cmd = null;
    try {
      cmd = parser.parse(options, args);
    } catch (ParseException e) {
      e.printStackTrace();
    }
    if(cmd.hasOption("h")) {
      HelpFormatter formatter = new HelpFormatter();
      formatter.printHelp( "ConverTor\n", options );
    }

    if(cmd.hasOption("f") && cmd.getOptionValue("f") != null) {
      String formatArgument = cmd.getOptionValue("f").toLowerCase();
      if (formatArgument.equals("avro")) {
        avro = true;
        parquet = false;
        format = formatArgument;
      }
      else if (formatArgument.equals("json")) {
        json = true;
        parquet = false;
        format = formatArgument;
      }
      else if (!formatArgument.equals("parquet"))  {
        HelpFormatter formatter = new HelpFormatter();
        formatter.printHelp( "ConverTor\n\n" +
                "Sorry, but " + cmd.getOptionValue("f") +
                " is not a valid format.\n\n", options );
      }
    }
    if(cmd.hasOption("s") && cmd.getOptionValue("s") != null) {
      suffix  = cmd.getOptionValue("s");
    }
    if(cmd.hasOption("i") && cmd.getOptionValue("i") != null) {
      inPath = cmd.getOptionValue("i");
    }
    if(cmd.hasOption("o") && cmd.getOptionValue("o") != null) {
      outPath = cmd.getOptionValue("o");
    }
    if(cmd.hasOption("v")) {
      verbose = true;
    }
    if(cmd.hasOption("p")) {
      pretty = true;
    }
    if(cmd.hasOption("c") && verbose && json) {
      compressed = true;
    }

    outputFileEnding = suffix + "." + format + ( compressed ? ".gz" : "");
    //  String outputPath = outPath + type + date + outputFileEnding;;


    //  TODO  remove after testing
    System.out.println("format = " + format);
    System.out.println("suffix = " + suffix);
    System.out.println("inPath = " + inPath);
    System.out.println("outPath = " + outPath);
    System.out.println("verbose = " + verbose);
    System.out.println("compressed JSON = " + compressed);
    System.out.println("pretty printed JSON = " + pretty);
    System.out.println("outputFileEnding = " + outputFileEnding);



    // SET UP ITERATOR OVER INPUT DESCRIPTORS
    DescriptorReader descriptorReader = DescriptorSourceFactory.createDescriptorReader();
    descriptorReader.addDirectory(new File(inPath));
    descriptorReader.setMaxDescriptorFilesInQueue(max);
    Iterator<DescriptorFile> descriptorFiles = descriptorReader.readDescriptors();


    // ITERATE OVER INPUT DESCRIPTORS AND INITIATE CONVERSION ACCORDING TO TYPE
    while (descriptorFiles.hasNext()) {
      //  initialize iterator
      DescriptorFile descriptorFile = descriptorFiles.next();
      //  handle exceptions
      if(null != descriptorFile.getException()){
        System.err.print(descriptorFile.getException()
                + "\n    in " + descriptorFile.getFileName() + "\n");
      }
      //  initiate conversion according to type
      for (Descriptor descriptor : descriptorFile.getDescriptors()) {

/*      //  relay
        if (descriptor instanceof RelayServerDescriptor) {
          writeOut(ConvertRelay.convert((RelayServerDescriptor) descriptor));
        }
        //  bridge
        if (descriptor instanceof BridgeServerDescriptor) {
          writeOut(ConvertBridge.convert((BridgeServerDescriptor) descriptor));
        }
        //  relayExtra
        if (descriptor instanceof RelayExtraInfoDescriptor) {
          writeOut(ConvertRelayExtra.convert((RelayExtraInfoDescriptor) descriptor));
        }
        //  bridgeExtra
        if (descriptor instanceof BridgeExtraInfoDescriptor) {
          writeOut(ConvertBridgeExtra.convert((BridgeExtraInfoDescriptor) descriptor));
        }
        //  relayVote
        if (descriptor instanceof RelayNetworkStatusVote) {
          writeOut(ConvertRelayVote.convert((RelayNetworkStatusVote) descriptor));
        }
        //  relayConsensus
        if (descriptor instanceof RelayNetworkStatusConsensus) {
          writeOut(ConvertRelayConsensus.convert((RelayNetworkStatusConsensus) descriptor));
        }
        //  bridgeStatus
        if (descriptor instanceof BridgeNetworkStatus) {
          writeOut(ConvertBridgeStatus.convert((BridgeNetworkStatus) descriptor));
        }
        //  tordnsel
        if (descriptor instanceof ExitList) {
          writeOut(ConvertExitList.convert((ExitList) descriptor));
        }
*/
        //  torperf
        if (descriptor instanceof TorperfResult) {
          appendToWriter(ConvertTorperf.convert((TorperfResult) descriptor));
        }
        //  handle unrecognized attributes
        if (!descriptor.getUnrecognizedLines().isEmpty()) {
          System.err.println("Unrecognized lines in "
                  + descriptorFile.getFileName() + ":");
          System.err.println(descriptor.getUnrecognizedLines());
          continue;
        }
      }
    }

    //  write to disk and clean up after all descriptors have been converted
    writeOut();

  } // end of method main



  /*
   *
   * COMMONS
   *
   */


  /*
   * file writing machinery
   */


  // a map to hold all writers per type + month
  // static Map<String, DataFileWriter> writerObjectsMap = new HashMap();
  static Map<String, Object> writerObjectsMap = new HashMap();


  public static void appendToWriter(Converted converted) throws IOException {
    // construct writer id
    String writerID = converted.type.name + "_" + converted.date;
    // get reference to writer
    Object writer = writerObjectsMap.get(writerID);
    // check if this writer does not already exists
    if (writer == null) {
      // create writerObject
      WriterObject writerObject = new WriterObject(converted.type, converted.date);
    }
    // append the converted descriptor to it
    assert writer != null;
    if (avro) {
      DataFileWriter avroWriter = (DataFileWriter) writer;
      avroWriter.append(converted.load);
    }
    else if (json) {



    }
    else { // parquet
      AvroParquetWriter parquetWriter = (AvroParquetWriter) writer;
      parquetWriter.write(converted.load);
    }
  }


  static class WriterObject {  // extends FileWriter / Closeable / FileWriter / DataFileWriter

    // Constructor
    WriterObject(DescriptorType descType, String date) throws IOException {

      String writerID = descType.name + "_" + date;
      File outputFile = new File(outPath + writerID + outputFileEnding);
      Schema schema = descType.avsc;

      if (avro) {

        // haplessly try to change Torperf to a variable
        Class X; // maybe cast to org.apache.avro.specific.SpecificRecordBase ?
        try {
          X = Class.forName("converTor." + descType.name + "." + descType.clasName);
        } catch (ClassNotFoundException c) {
          c.printStackTrace();
        }

        // frustratedly settle for Torperf for now
        DatumWriter<Torperf> avroDatumWriter = new SpecificDatumWriter<>(Torperf.class);
        DataFileWriter<Torperf> dataFileWriter = new DataFileWriter<>(avroDatumWriter);
        dataFileWriter.create(schema, outputFile);

        // add the created writer (specific to type and date) to the map that holds them all
        writerObjectsMap.put(writerID, dataFileWriter);
        /*
        Writer torperfDatumWriter = new SpecificDatumWriter<Torperf>(Torperf.class);
        torperfDatumWriter = new SpecificDatumWriter<Torperf>(Torperf.class);
        torperfDataFileWriter = new DataFileWriter<Torperf>(torperfDatumWriter);
        dataFileWriters.add(torperfDataFileWriter);
        Torperf torperf = ConvertTorperf.convert((TorperfResult) descriptor);
        torperfDataFileWriter.create(torperf.getSchema(), torperfAvroFile);
        torperfDataFileWriter.append(torperf);
        */



      }
      else if (json) {

        // Json.ObjectWrite   (where was that from?)
        // nobody seems to use this - queries return no results
        // it seems very low level
        Json.ObjectWriter aJsonDatumWriter = new Json.ObjectWriter();
        aJsonDatumWriter.setSchema(schema);




        // todo   ask on parquet-mr mailinglist if specific mapping is supported
        //        if yes, how (please)
        //        prepare switch to generic mapping
        //        which will also solve some other problems with
        //          nested converter code
        //          referencing classes in WriterObject
        //          all examples using generic mapping





        // or like this - https://gist.github.com/hammer/76996fb8426a0ada233e
        // together with this- http://www.javased.com/?api=org.apache.avro.io.EncoderFactory - example 10
        DatumWriter<Torperf> jsonDatumWriter = new SpecificDatumWriter<>(Torperf.class);

        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        JsonEncoder jsonEncoder = EncoderFactory.get().jsonEncoder(schema, baos);


        // add the created writer (specific to type and date) to the map that holds them all
        writerObjectsMap.put(writerID, jsonWriter);


        jsonDatumWriter.write(descType.load, jsonEncoder);
        jsonEncoder.flush();
        System.out.println("JSON encoded record: " + baos); // is this additionally? didn't we write to a file? ah, what file...








        // problems with JSON number conversion? see: https://docs.oracle.com/cd/E26161_02/html/GettingStartedGuide/jsonbinding-overview.html
      }
      else { // parquet, uses parquet-mr
        // Hadoop Definitive Guide p.375
        // Path is a Hadoop FileSystem command - not sure how (or if) this works on a normal file system
        Path parquetOutput = new Path(outPath + writerID + outputFileEnding);
        AvroParquetWriter<SpecificRecord> parquetWriter = new AvroParquetWriter<>(parquetOutput, schema);

        // add the created writer (specific to type and date) to the map that holds them all
        writerObjectsMap.put(writerID, parquetWriter);
      }
    }
  }








  public static void writeOut() throws IOException {
    for ( Object writer : writerObjectsMap.values()) {
      if (avro) {
        DataFileWriter avroWriter = (DataFileWriter) writer;
        avroWriter.close();
      }
      else if (json) {

      }
      else { // parquet
        AvroParquetWriter parquetWriter = (AvroParquetWriter) writer;
        parquetWriter.close();
      }
    }
  }


  /*
   * object to return from conversion
   */

  static class Converted  {
    DescriptorType type;
    String date;
    SpecificRecordBase load;
  }


  /*
   * common to all converters
   */

  static class Converter {

    /*  generic key/value objects for verbose output  */
    static class StringInt {
      String key;
      int val;
      StringInt(String key, int val) {
        this.key = key;
        this.val = val;
      }
    }
    static class StringLong {
      String key;
      Long val;
      StringLong(String key, Long val) {
        this.key = key;
        this.val = val;
      }
    }
    static class StringDouble {
      String key;
      Double val;
      StringDouble(String key, Double val) {
        this.key = key;
        this.val = val;
      }
    }

    /* ExtraInfo and Stats objects used in extra info descriptors */
    static class ExtraInfo {
      String nickname;
      String fingerprint;
    }
    static class Stats {
      String date;
      Long interval = 86400L;   // TODO  is it really useful to insert the default here?
    }

    /*  Serialize "read-history" and "write-history" lines  */
    static class BandwidthHistory {
      String date; // format is YYYY-MM-DD HH:MM:SS
      long interval; // seconds
      Collection<Long> bytes;
    }
    /*  Convert read or write history  */
    static BandwidthHistory convertBandwidthHistory(org.torproject.descriptor.BandwidthHistory hist) {
      BandwidthHistory bandwidthHistory = new BandwidthHistory();
      bandwidthHistory.date = dateTimeFormat.format(hist.getHistoryEndMillis());
      bandwidthHistory.interval = hist.getIntervalLength();
      bandwidthHistory.bytes = hist.getBandwidthValues().values();
      return bandwidthHistory;
    }
    /*  Date/time formatter  */
    static final String dateTimePattern = "yyyy-MM-dd HH:mm:ss";
    static final Locale dateTimeLocale = Locale.US;
    static final TimeZone dateTimezone = TimeZone.getTimeZone("UTC");
    static DateFormat dateTimeFormat;
    static {
      dateTimeFormat = new SimpleDateFormat(dateTimePattern, dateTimeLocale);
      dateTimeFormat.setLenient(false);
      dateTimeFormat.setTimeZone(dateTimezone);
    }
  }




  /*
   *
   * THE ACTUAL CONVERTERS, ONE PER DESCRIPTOR TYPE, NINE IN TOTAL
   *
   */


  //  tordnsel
  /*
  static class ConvertTordnsel extends Converter {

    List<ExitNode> exit_nodes;

    static class ExitNode {
      String fingerprint;
      String published;
      String last_status;
      // List<Exit> exit_list;
      Object exit_list;
    }

    static class Exit {
      String ip;
      String date;
    }


    static Tordnsel convert(ExitList desc) {


//      Tordnsel tordnsel = new Tordnsel();
//      for (String annotation : desc.getAnnotations()) {
//        tordnsel.setDescriptorType(annotation.substring("@type ".length()));
//      }
//      tordnsel.setDownloaded(desc.getDownloadedMillis();
//      tordnsel.setExitNodes(AvroExitNodes.convert( ));


//      //how to populate nested records/arrays/maps
//      https://stackoverflow.com/questions/5480043/question-populating-nested-records-in-avro-using-a-genericrecord
//
//      Schema  sch =  Schema.parse(schemaFile);
//      DataFileWriter<GenericRecord> fw = new DataFileWriter<GenericRecord>(new GenericDatumWriter<GenericRecord>()).create(sch, new File(outFile));
//      GenericRecord r = new GenericData.Record(sch);
//      r.put(“firstName”, “John”);
//      fw.append(r);
//
//      GenericRecord t = new GenericData.Record(sch.getField("address").schema());
//      t.put("city","beijing");
//      r.put("address",t);


//      tordnsel.exit_nodes = new ArrayList<>();
//      if (desc.getEntries() != null && !desc.getEntries().isEmpty()) {
//        for(ExitList.Entry exitEntry : desc.getEntries()) {
//          ExitNode exitNode = new ExitNode();
//          exitNode.fingerprint = exitEntry.getFingerprint();
//          exitNode.published = dateTimeFormat.format(exitEntry.getPublishedMillis());
//          exitNode.last_status = dateTimeFormat.format(exitEntry.getLastStatusMillis());
//          if (exitEntry.getExitAddresses() != null && !exitEntry.getExitAddresses().isEmpty()) {
//            *//**//* jagged *//**//*
//            exitNode.exit_list = new HashMap<String, String>();
//            HashMap<String, String> jaggedList = new HashMap<>();
//            for (Map.Entry<String, Long> exitAddress : exitEntry.getExitAddresses().entrySet()) {
//              jaggedList.put(exitAddress.getKey(), dateTimeFormat.format(exitAddress.getValue()));
//            }
//            exitNode.exit_list = jaggedList;
//            if (verbose) {
//              exitNode.exit_list_VERBOSE = new ArrayList<Exit>();
//              ArrayList<Exit> flatExit = new ArrayList<>();
//              for (Map.Entry<String, Long> exitAddress : exitEntry.getExitAddresses().entrySet()) {
//                Exit exit = new Exit();
//                exit.ip = exitAddress.getKey();
//                exit.date = dateTimeFormat.format(exitAddress.getValue());
//                flatExit.add(exit);
//              }
//              exitNode.exit_list = flatExit;
//            }
//          }
//          tordnsel.exit_nodes.add(exitNode);
//        }
//      }


//      //  inner class exitList
//      class AvroExitNodes extends AvroDescriptor {
//        Torperf convert(TorperfResult desc) {
//          Torperf torperf = new Torperf();
//          torperf.setDescriptorType("torperf 1.0");
//          torperf.setSource(desc.getSource());
//          torperf.setFilesize(desc.getFileSize());
//          return torperf;
//        }
//      }

      return tordnsel;
    }

  } */


  //  torperf
  //  SPECIFIC with CONSTRUCTOR
  static class ConvertTorperf extends Converter {
    static Converted convert(TorperfResult desc) {

      Torperf conversion = new Torperf();

      conversion.setDescriptorType("torperf 1.0");
      conversion.setSource(desc.getSource());
      conversion.setFilesize(desc.getFileSize());
      conversion.setStart(desc.getStartMillis());
      conversion.setSocket(desc.getSocketMillis());
      conversion.setConnect(desc.getConnectMillis());
      conversion.setNegotiate(desc.getNegotiateMillis());
      conversion.setRequest(desc.getRequestMillis());
      conversion.setResponse(desc.getResponseMillis());
      conversion.setDatarequest(desc.getDataRequestMillis());
      conversion.setDataresponse(desc.getDataResponseMillis());
      conversion.setDatacomplete(desc.getDataCompleteMillis());
      conversion.setWritebytes(desc.getWriteBytes());
      conversion.setReadbytes(desc.getReadBytes());
      conversion.setDidtimeout(desc.didTimeout());
      if (desc.getDataPercentiles() != null && !desc.getDataPercentiles().isEmpty()) {
        conversion.setDataperc10(desc.getDataPercentiles().get(10));
        conversion.setDataperc20(desc.getDataPercentiles().get(20));
        conversion.setDataperc30(desc.getDataPercentiles().get(30));
        conversion.setDataperc40(desc.getDataPercentiles().get(40));
        conversion.setDataperc50(desc.getDataPercentiles().get(50));
        conversion.setDataperc60(desc.getDataPercentiles().get(60));
        conversion.setDataperc70(desc.getDataPercentiles().get(70));
        conversion.setDataperc80(desc.getDataPercentiles().get(80));
        conversion.setDataperc90(desc.getDataPercentiles().get(90));
      }
      conversion.setLaunch(desc.getLaunchMillis());
      conversion.setUsedAt(desc.getUsedAtMillis());
      if (desc.getPath() != null && !desc.getPath().isEmpty()) {
        conversion.setPath(desc.getPath());
      }
      conversion.setBuildtimes(desc.getBuildTimes());
      conversion.setTimeout(desc.getTimeout());
      conversion.setQuantile(desc.getQuantile());
      conversion.setCircId(desc.getCircId());
      conversion.setUsedBy(desc.getUsedBy());

      Converted converted = new Converted();
      converted.type = torperfType;
      converted.date = dateTimeFormat.format(desc.getStartMillis()).substring(0,7);
      converted.load = conversion;
      return converted;
    }
  }


  //  SPECIFIC with BUILDER
  /*
  static class SpecificBuilderAvroTorperfDescriptor extends AvroDescriptor {
    static Torperf convert(TorperfResult desc) {

      //  tests needed below
      boolean percentiles = desc.getDataPercentiles() != null &&
                            !desc.getDataPercentiles().isEmpty();
      boolean path = desc.getPath() != null && !desc.getPath().isEmpty();

      Torperf torperf;
      torperf = Torperf.newBuilder()

              .setDescriptorType("torperf 1.0")
              .setSource(desc.getSource())
              .setFilesize(desc.getFileSize())
              .setStart(desc.getStartMillis())
              .setSocket(desc.getSocketMillis())
              .setConnect(desc.getConnectMillis())
              .setNegotiate(desc.getNegotiateMillis())
              .setRequest(desc.getRequestMillis())
              .setResponse(desc.getResponseMillis())
              .setDatarequest(desc.getDataRequestMillis())
              .setDataresponse(desc.getDataResponseMillis())
              .setDatacomplete(desc.getDataCompleteMillis())
              .setWritebytes(desc.getWriteBytes())
              .setReadbytes(desc.getReadBytes())
              .setDidtimeout(desc.didTimeout())
              .setDataperc10(percentiles ? desc.getDataPercentiles().get(10) : null)
              .setDataperc20(percentiles ? desc.getDataPercentiles().get(20) : null)
              .setDataperc30(percentiles ? desc.getDataPercentiles().get(30) : null)
              .setDataperc40(percentiles ? desc.getDataPercentiles().get(40) : null)
              .setDataperc50(percentiles ? desc.getDataPercentiles().get(50) : null)
              .setDataperc60(percentiles ? desc.getDataPercentiles().get(60) : null)
              .setDataperc70(percentiles ? desc.getDataPercentiles().get(70) : null)
              .setDataperc80(percentiles ? desc.getDataPercentiles().get(80) : null)
              .setDataperc90(percentiles ? desc.getDataPercentiles().get(90) : null)
              .setLaunch(desc.getLaunchMillis())
              .setUsedAt(desc.getUsedAtMillis())
              .setPath(path ? desc.getPath() : null)
              .setBuildtimes(desc.getBuildTimes())
              .setTimeout(desc.getTimeout())
              .setQuantile(desc.getQuantile())
              .setCircId(desc.getCircId())
              .setUsedBy(desc.getUsedBy())

              .build();

      return torperf;
    }
  }
  */

  //  GENERIC
  /*  delete after settling for SPECIIFIC mapping
  static class GenericAvroTorperfDescriptor extends AvroDescriptor {
    static GenericRecord convert(GenericRecord torperf, TorperfResult desc) {

      //      manually check types for consistency
      //        yes, manually! one might think "this is java", but still...
      //        BUT there is a validatingEncoder that checks types
      //            against the schema while encoding

      torperf.put("descriptor_type", "torperf 1.0");
      torperf.put("source", desc.getSource());
      torperf.put("filesize", desc.getFileSize());
      torperf.put("start", dateTimeFormat.format(desc.getStartMillis()));
      torperf.put("socket", dateTimeFormat.format(desc.getSocketMillis()));
      torperf.put("connect", dateTimeFormat.format(desc.getConnectMillis()));
      torperf.put("negotiate", dateTimeFormat.format(desc.getNegotiateMillis()));
      torperf.put("request", dateTimeFormat.format(desc.getRequestMillis()));
      torperf.put("response", dateTimeFormat.format(desc.getResponseMillis()));
      torperf.put("datarequest", dateTimeFormat.format(desc.getDataRequestMillis()));
      torperf.put("dataresponse", dateTimeFormat.format(desc.getDataResponseMillis()));
      torperf.put("datacomplete", dateTimeFormat.format(desc.getDataCompleteMillis()));
      torperf.put("writebytes", desc.getWriteBytes());
      torperf.put("readbytes", desc.getReadBytes());
      torperf.put("didtimeout", desc.didTimeout());
      if (desc.getDataPercentiles() != null && !desc.getDataPercentiles().isEmpty()) {
        torperf.put("dataperc10", desc.getDataPercentiles().get(10));
        torperf.put("dataperc20", desc.getDataPercentiles().get(20));
        torperf.put("dataperc30", desc.getDataPercentiles().get(30));
        torperf.put("dataperc40", desc.getDataPercentiles().get(40));
        torperf.put("dataperc50", desc.getDataPercentiles().get(50));
        torperf.put("dataperc60", desc.getDataPercentiles().get(60));
        torperf.put("dataperc70", desc.getDataPercentiles().get(70));
        torperf.put("dataperc80", desc.getDataPercentiles().get(80));
        torperf.put("dataperc90", desc.getDataPercentiles().get(90));
      }
      torperf.put("launch", desc.getLaunchMillis());
      torperf.put("used_at", desc.getUsedAtMillis());
      torperf.put("path", desc.getPath());
      torperf.put("buildtimes", desc.getBuildTimes());
      torperf.put("timeout", desc.getTimeout());
      torperf.put("quantile", desc.getQuantile());
      torperf.put("circ_id", desc.getCircId());
      torperf.put("used_by", desc.getUsedBy());

      return torperf;
    }
  }
  */



}